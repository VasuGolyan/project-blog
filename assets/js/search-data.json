{
  
    
        "post0": {
            "title": "Distinguish Your Own Digits (DYOD) - A NN Model Exercise",
            "content": "This notebook will distinguish betwwen two handwritten digits based on training from MNIST database. . Importing all the necessary libraries . %load_ext autoreload %autoreload 2 . %matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd . !pip install mnist . Requirement already satisfied: mnist in c: anaconda3 lib site-packages (0.2.2) Requirement already satisfied: numpy in c: anaconda3 lib site-packages (from mnist) (1.18.1) . Preparing the Data . import mnist . train_images = mnist.train_images() train_labels = mnist.train_labels() . train_images.shape, train_labels.shape . ((60000, 28, 28), (60000,)) . test_images = mnist.test_images() test_labels = mnist.test_labels() . test_images.shape, test_labels.shape . ((10000, 28, 28), (10000,)) . image_index = 7776 # You may select anything up to 60,000 print(train_labels[image_index]) plt.imshow(train_images[image_index], cmap=&#39;Greys&#39;) . 2 . &lt;matplotlib.image.AxesImage at 0x1dd4b0c9c88&gt; . Filter data to get 3 and 8 out . train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . We normalize the pizel values in the 0 to 1 range . X_train = X_train/255. X_test = X_test/255. . And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8) . y_train = 1*(y_train==3) y_test = 1*(y_test==3) . X_train.shape, X_test.shape . ((11982, 28, 28), (1984, 28, 28)) . We reshape the data to flatten the image pixels into a set of features or co-variates: . X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) X_train.shape, X_test.shape . ((11982, 784), (1984, 784)) . #libraries from kudzu.data import Data, Dataloader, Sampler from kudzu.callbacks import AccCallback from kudzu.loss import MSE from kudzu.layer import Affine, Sigmoid from kudzu.model import Model from kudzu.optim import GD from kudzu.train import Learner from kudzu.callbacks import ClfCallback from kudzu.layer import Sigmoid from kudzu.layer import Relu . class Config: pass config = Config() config.lr = 0.001 config.num_epochs = 250 config.bs = 50 . #data initialization data = Data(X_train, y_train.reshape(-1,1)) loss = MSE() opt = GD(config.lr) sampler = Sampler(data, config.bs, shuffle=True) dl = Dataloader(data, sampler) . #containers training_xdata = X_train testing_xdata = X_test training_ydata = y_train.reshape(-1,1) testing_ydata = y_test.reshape(-1,1) . #NN model initialization layers = [Affine(&quot;first&quot;, 784, 100), Relu(&quot;first&quot;), Affine(&quot;second&quot;, 100, 100), Relu(&quot;second&quot;), Affine(&quot;third&quot;, 100, 2), Affine(&quot;final&quot;, 2, 1), Sigmoid(&quot;final&quot;)] model_neural = Model(layers) model_logistic = Model([Affine(&quot;logits&quot;, 784, 1), Sigmoid(&quot;sigmoid&quot;)]) learner1 = Learner(loss, model_neural, opt, config.num_epochs) acc1 = ClfCallback(learner1, config.bs, training_xdata , testing_xdata, training_ydata, testing_ydata) learner1.set_callbacks([acc1]) . #learner call learner1.train_loop(dl) . Epoch 0 Loss 0.2736260500679626 train accuracy is: 0.4356534802203305, test accuracy is 0.4596774193548387 Epoch 10 Loss 0.10898970213675427 train accuracy is: 0.9128693039559339, test accuracy is 0.9269153225806451 Epoch 20 Loss 0.061438693976068555 train accuracy is: 0.9376564847270906, test accuracy is 0.9465725806451613 Epoch 30 Loss 0.046799196765135075 train accuracy is: 0.9505090969788015, test accuracy is 0.9561491935483871 Epoch 40 Loss 0.039874034691351735 train accuracy is: 0.9554331497245869, test accuracy is 0.9642137096774194 Epoch 50 Loss 0.03583555451240939 train accuracy is: 0.9591887831747622, test accuracy is 0.9657258064516129 Epoch 60 Loss 0.033149081283470806 train accuracy is: 0.9621932899349024, test accuracy is 0.9657258064516129 Epoch 70 Loss 0.031171271947087378 train accuracy is: 0.9640293773994325, test accuracy is 0.9672379032258065 Epoch 80 Loss 0.029622195845065855 train accuracy is: 0.9657820063428476, test accuracy is 0.9672379032258065 Epoch 90 Loss 0.028363506333614463 train accuracy is: 0.9678684693707228, test accuracy is 0.9682459677419355 Epoch 100 Loss 0.02730636679153363 train accuracy is: 0.9690368886663329, test accuracy is 0.9702620967741935 Epoch 110 Loss 0.02639704377322129 train accuracy is: 0.9694541812719079, test accuracy is 0.9722782258064516 Epoch 120 Loss 0.025596392894720783 train accuracy is: 0.970288766483058, test accuracy is 0.9717741935483871 Epoch 130 Loss 0.024883179582617758 train accuracy is: 0.971457185778668, test accuracy is 0.9712701612903226 Epoch 140 Loss 0.02423449780813126 train accuracy is: 0.972291770989818, test accuracy is 0.9712701612903226 Epoch 150 Loss 0.023642400132849118 train accuracy is: 0.9727090635953931, test accuracy is 0.9712701612903226 Epoch 160 Loss 0.02309271487127649 train accuracy is: 0.9737940243698882, test accuracy is 0.9707661290322581 Epoch 170 Loss 0.022580064438461795 train accuracy is: 0.9738774828910032, test accuracy is 0.9717741935483871 Epoch 180 Loss 0.022087778374958667 train accuracy is: 0.9746286095810383, test accuracy is 0.9727822580645161 Epoch 190 Loss 0.021614784251382343 train accuracy is: 0.9756301118344183, test accuracy is 0.9732862903225806 Epoch 200 Loss 0.02118366949591829 train accuracy is: 0.9761308629611083, test accuracy is 0.9737903225806451 Epoch 210 Loss 0.020754582769080612 train accuracy is: 0.9772158237356035, test accuracy is 0.9737903225806451 Epoch 220 Loss 0.020348892651367842 train accuracy is: 0.9777165748622935, test accuracy is 0.9737903225806451 Epoch 230 Loss 0.01996731932006799 train accuracy is: 0.9782173259889835, test accuracy is 0.9737903225806451 Epoch 240 Loss 0.01959853968485756 train accuracy is: 0.9781338674678685, test accuracy is 0.9737903225806451 . 0.028509718966256783 . #LR model initalization learner2 = Learner(loss, model_logistic, opt, config.num_epochs) acc2 = ClfCallback(learner2, config.bs, training_xdata , testing_xdata, training_ydata, testing_ydata) learner2.set_callbacks([acc2]) . #learner call learner2.train_loop(dl) . Epoch 0 Loss 0.2635071287509881 train accuracy is: 0.5810382240026707, test accuracy is 0.5372983870967742 Epoch 10 Loss 0.10514060051205106 train accuracy is: 0.9100317142380236, test accuracy is 0.9133064516129032 Epoch 20 Loss 0.07995434244529664 train accuracy is: 0.9270572525454849, test accuracy is 0.9349798387096774 Epoch 30 Loss 0.0686117715836812 train accuracy is: 0.9368218995159405, test accuracy is 0.9445564516129032 Epoch 40 Loss 0.061855314486321346 train accuracy is: 0.9406609914872308, test accuracy is 0.9506048387096774 Epoch 50 Loss 0.05728031864902006 train accuracy is: 0.9444166249374061, test accuracy is 0.953125 Epoch 60 Loss 0.05393361606509065 train accuracy is: 0.9465865464863963, test accuracy is 0.9536290322580645 Epoch 70 Loss 0.05135813240139899 train accuracy is: 0.9479218828242364, test accuracy is 0.9566532258064516 Epoch 80 Loss 0.04929724135261386 train accuracy is: 0.9503421799365716, test accuracy is 0.9586693548387096 Epoch 90 Loss 0.04760339542371465 train accuracy is: 0.9514271407110666, test accuracy is 0.9606854838709677 Epoch 100 Loss 0.04617781687185664 train accuracy is: 0.9525121014855616, test accuracy is 0.9621975806451613 Epoch 110 Loss 0.04495648773827633 train accuracy is: 0.9532632281755967, test accuracy is 0.9627016129032258 Epoch 120 Loss 0.04389470594328971 train accuracy is: 0.9536805207811717, test accuracy is 0.9637096774193549 Epoch 130 Loss 0.04296016991172947 train accuracy is: 0.9545151059923218, test accuracy is 0.9647177419354839 Epoch 140 Loss 0.04212959799505519 train accuracy is: 0.9553496912034719, test accuracy is 0.9642137096774194 Epoch 150 Loss 0.04138366377287864 train accuracy is: 0.9559339008512769, test accuracy is 0.9642137096774194 Epoch 160 Loss 0.0407100558707365 train accuracy is: 0.957018861625772, test accuracy is 0.9642137096774194 Epoch 170 Loss 0.04009730731392845 train accuracy is: 0.957352695710232, test accuracy is 0.9652217741935484 Epoch 180 Loss 0.03953559048300741 train accuracy is: 0.957603071273577, test accuracy is 0.9657258064516129 Epoch 190 Loss 0.0390201983233393 train accuracy is: 0.958103822400267, test accuracy is 0.9662298387096774 Epoch 200 Loss 0.03854244600549152 train accuracy is: 0.9586880320480721, test accuracy is 0.9657258064516129 Epoch 210 Loss 0.03810000382484748 train accuracy is: 0.9590218661325322, test accuracy is 0.9667338709677419 Epoch 220 Loss 0.03768685027812864 train accuracy is: 0.9592722416958771, test accuracy is 0.9667338709677419 Epoch 230 Loss 0.037302405439647114 train accuracy is: 0.9596060757803372, test accuracy is 0.9667338709677419 Epoch 240 Loss 0.03694092276161223 train accuracy is: 0.9598564513436821, test accuracy is 0.9667338709677419 . 0.02761287046103677 . #Comparative Stats plt.figure(figsize = (8,5)) plt.plot(acc1.val_accuracies, &#39;g-&#39;, label = &quot;Neural_Network - Val_Accuracy&quot;) plt.plot(acc1.accuracies, &#39;b-&#39;, label = &quot;Neural_Network - Accuracies&quot;) plt.plot(acc2.val_accuracies, &#39;r-&#39;, label = &quot;Logistic_Regr - Val_Accuracies&quot;) plt.plot(acc2.accuracies, &#39;y-&#39;, label = &quot;Logistic_Regr - Accuracies&quot;) plt.ylim(0.85,1) #check plt.legend() . &lt;matplotlib.legend.Legend at 0x1dd67022148&gt; . #2D Output model_new = Model(layers[:-2]) plot_testing = model_new(testing_xdata) plt.figure(figsize=(8,7)) #blows up plt.scatter(plot_testing[:,0], plot_testing[:,1], alpha = 0.1, c = y_test.ravel()); . #isolating affine -&gt; sigmoid model_prob = Model(layers[-2:]) xgrid = np.linspace(-4.5, 2, 100) ygrid = np.linspace(-8, 8, 100) xg, yg = np.meshgrid(xgrid, ygrid) xg_interim = np.ravel(xg) #cant do; Check later yg_interim = np.ravel(yg) X_interim = np.vstack((xg_interim, yg_interim)) ## Please note vstack takes in a tuple X = X_interim.T probability_contour = model_prob(X).reshape(100,100) . plt.figure(figsize=(8,7)) plt.scatter(plot_testing[:,0], plot_testing[:,1], alpha = 0.1, c = y_test.ravel()) contours = plt.contour(xg,yg,probability_contour) plt.clabel(contours, inline=True); . Inference: . From the graph we see that the neural network has better accuracy over the logistic regression learning model. Also, we can see hints of overfitting in the model, as the validation accruracy and test accuracy cross each other and then diverge .",
            "url": "https://vasugolyan.github.io/project-blog/2020/08/05/DYOD-Neural-Network.html",
            "relUrl": "/2020/08/05/DYOD-Neural-Network.html",
            "date": " • Aug 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "COVID-19 India Tracker",
            "content": "Tracking coronavirus total cases, deaths and new cases in India . A Matplotlib Overview by Vasu Golyan . #collapse update = summary[&#39;updated&#39;] cases = summary[&#39;Cases&#39;] new = summary[&#39;Cases (+)&#39;] deaths = summary[&#39;Deaths&#39;] dnew = summary[&#39;Deaths (+)&#39;] overview = &#39;&#39;&#39; &lt;!-- ####### HTML!! #########--&gt; &lt;h1 style=&quot;color: #5e9ca0; text-align: center;&quot;&gt;India&lt;/h1&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Last update: &lt;strong&gt;{update}&lt;/strong&gt;&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed cases:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{cases} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{new}&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed deaths:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{deaths} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{dnew}&lt;/span&gt;)&lt;/p&gt; &#39;&#39;&#39; html = HTML(overview.format(update=update, cases=cases,new=new,deaths=deaths,dnew=dnew)) display(html) . . India . Last update: 28th July, 2020 . Confirmed cases: . 1514800 (+49001) . Confirmed deaths: . 34121 (+770) . #collapse dft_ct_new_cases.head() . . 14-Mar-20 15-Mar-20 16-Mar-20 17-Mar-20 18-Mar-20 19-Mar-20 20-Mar-20 21-Mar-20 22-Mar-20 23-Mar-20 ... 21-Jul-20 22-Jul-20 23-Jul-20 24-Jul-20 25-Jul-20 26-Jul-20 27-Jul-20 28-Jul-20 dt_today dt_yday . states . Andhra Pradesh 0 | 0 | 0 | 0 | 0 | 2 | 0 | 2 | 1 | 1 | ... | 4944 | 6045 | 7998 | 8147 | 7813 | 7627 | 6051 | 7948 | 0 | -7948 | . Maharashtra 0 | 18 | 6 | 3 | 3 | 4 | 4 | 12 | 10 | 23 | ... | 8336 | 10576 | 9895 | 9615 | 9251 | 9431 | 7924 | 7717 | 0 | -7717 | . Tamil Nadu 0 | 0 | 0 | 0 | 1 | 1 | 0 | 3 | 3 | 3 | ... | 4965 | 5849 | 6472 | 6785 | 6988 | 6986 | 6993 | 6972 | 0 | -6972 | . Karnataka 0 | 0 | 1 | 2 | 5 | 1 | 0 | 5 | 6 | 7 | ... | 3649 | 4764 | 5030 | 5007 | 5072 | 5199 | 5324 | 5536 | 0 | -5536 | . Uttar Pradesh 0 | 1 | 0 | 2 | 2 | 3 | 4 | 4 | 2 | 2 | ... | 2128 | 2300 | 2516 | 2667 | 2971 | 3246 | 3505 | 3458 | 0 | -3458 | . 5 rows × 139 columns . #collapse ax = [] fig = plt.figure(figsize = (16,20)) gs = fig.add_gridspec(n+2, 3) # gs = fig.add_gridspec(2, 3) ax1 = fig.add_subplot(gs[0, :]) ef = df.loc[&#39;Total&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax1.bar(ef.date,ef.Total,alpha=0.3,color=&#39;#007acc&#39;) ax1.plot(ef.date,ef.Total , marker=&quot;o&quot;, color=&#39;#007acc&#39;) ax1.xaxis.set_major_locator(mdates.WeekdayLocator()) ax1.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax1.text(0.02, 0.5,&#39;India daily case count&#39;, transform = ax1.transAxes, fontsize=25); ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) ax2 = fig.add_subplot(gs[1,0]) ef = df.loc[&#39;Maharashtra&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax2.bar(ef.date, ef.Maharashtra,color = &#39;#007acc&#39;,alpha=0.5) ax2.xaxis.set_major_locator(mdates.WeekdayLocator()) ax2.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax2.set_xticks(ax2.get_xticks()[::3]) maxyval = ef.Maharashtra.max() ax2.set_ylim([0,maxyval]) ax2.text(0.05, 0.5,&#39;Maharashtra&#39;, transform = ax2.transAxes, fontsize=20); ax2.spines[&#39;right&#39;].set_visible(False) ax2.spines[&#39;top&#39;].set_visible(False) ax3 = fig.add_subplot(gs[1,1]) ef = df.loc[&#39;Tamil Nadu&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax3.bar(ef.date, ef[&#39;Tamil Nadu&#39;],color = &#39;#007acc&#39;,alpha=0.5,) ax3.xaxis.set_major_locator(mdates.WeekdayLocator()) ax3.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax3.set_xticks(ax3.get_xticks()[::3]) ax3.text(0.05, 0.5,&#39;Tamil Nadu&#39;, transform = ax3.transAxes, fontsize=20); ax3.spines[&#39;right&#39;].set_visible(False) ax3.spines[&#39;top&#39;].set_visible(False) ax4 = fig.add_subplot(gs[1,2]) ef = df.loc[&#39;Delhi&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax4.bar(ef.date, ef.Delhi,color = &#39;#007acc&#39;,alpha=0.5) ax4.set_xticks([]) ax4.xaxis.set_major_locator(mdates.WeekdayLocator()) ax4.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax4.set_xticks(ax4.get_xticks()[::3]) ax4.spines[&#39;right&#39;].set_visible(False) ax4.spines[&#39;top&#39;].set_visible(False) ax4.text(0.05, 0.5,&#39;Delhi&#39;, transform = ax4.transAxes, fontsize=20) for i in range(n): ax.append(fig.add_subplot(gs[i+2,:])) ef = df.iloc[i+3].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax[i].bar(ef.date,ef.iloc[:,-1],color = &#39;#007acc&#39;,alpha=0.3) ax[i].plot(ef.date,ef.iloc[:,-1],marker=&#39;o&#39;,color=&#39;#007acc&#39;) ax[i].text(0.02,0.5,f&#39;{ef.columns.values[-1]}&#39;,transform = ax[i].transAxes, fontsize = 20); ax[i].xaxis.set_major_locator(mdates.WeekdayLocator()) ax[i].xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax[i].set_ylim([0,7000]) ax[i].spines[&#39;right&#39;].set_visible(False) ax[i].spines[&#39;top&#39;].set_visible(False) plt.tight_layout() . . #collapse print(df_table.to_string(index=False)) . . states Cases PCases Deaths PDeaths Cases (+) Deaths (+) Fatality Rate Maharashtra 391440 383723 14164 13882 7717 282 3.62 Tamil Nadu 227688 220716 3659 3571 6972 88 1.61 Delhi 132275 131219 3881 3853 1056 28 2.93 Andhra Pradesh 110297 102349 1148 1090 7948 58 1.04 Karnataka 107001 101465 2064 1962 5536 102 1.93 Uttar Pradesh 73951 70493 1497 1456 3458 41 2.02 West Bengal 62964 60830 1449 1411 2134 38 2.30 Gujarat 57982 56874 2372 2348 1108 24 4.09 Telangana 57142 55532 480 471 1610 9 0.84 Bihar 43591 41111 269 255 2480 14 0.62 Rajasthan 38636 37564 644 633 1072 11 1.67 Assam 34846 33475 92 90 1371 2 0.26 Haryana 32876 32127 406 397 749 9 1.23 Madhya Pradesh 29217 28589 831 821 628 10 2.84 Orissa 28107 26892 189 181 1215 8 0.67 Kerala 20895 19728 68 64 1167 4 0.33 Jammu and Kashmir 18879 18390 333 321 489 12 1.76 Punjab 14378 13769 336 318 609 18 2.34 Jharkhand 9563 8803 94 90 760 4 0.98 Goa 5287 5119 36 36 168 0 0.68 Tripura 4287 4066 21 17 221 4 0.49 Pondicherry 3013 2874 47 43 139 4 1.56 Himachal Pradesh 2330 2270 13 13 60 0 0.56 Manipur 2317 2286 0 0 31 0 0.00 Nagaland 1460 1385 4 5 75 0 0.27 Arunachal Pradesh 1330 1239 3 3 91 0 0.23 Chandigarh 934 910 14 14 24 0 1.50 Meghalaya 779 738 5 5 41 0 0.64 Sikkim 592 568 1 1 24 0 0.17 Mizoram 384 361 0 0 23 0 0.00 Andaman and Nicobar Islands 359 334 1 1 25 0 0.28 Daman and Diu 0 0 0 0 0 0 NaN Lakshadweep 0 0 0 0 0 0 NaN .",
            "url": "https://vasugolyan.github.io/project-blog/2020/08/01/COVID19-India-Tracker.html",
            "relUrl": "/2020/08/01/COVID19-India-Tracker.html",
            "date": " • Aug 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://vasugolyan.github.io/project-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vasugolyan.github.io/project-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}